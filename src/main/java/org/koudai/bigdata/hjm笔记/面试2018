1 kafka集群的规模,消费速度是多少?
    一般中小型公司是10个节点,每秒20M左右

2 hdfs上传文件流程
   答：这里描述的 是一个256M的文件上传过程
   ① 由客户端 向 NameNode节点节点 发出请求
   ②NameNode 向Client返回可以可以存数据的 DataNode 这里遵循机架感应原则
   ③客户端 首先 根据返回的信息 先将 文件分块（Hadoop2.X版本 每一个block为 128M 而之前的版本为 64M
   ④然后通过那么Node返回的DataNode信息 直接发送给DataNode 并且是 流式写入  同时 会复制到其他两台机器
   ⑤dataNode 向 Client通信 表示已经传完 数据块 同时向NameNode报告 ⑥依照上面（④到⑤）的原理将 所有的数
     据块都上传结束 向 NameNode 报告 表明 已经传完所有的数据块 。

3.讲述一下mapreduce的流程
首先是 Mapreduce经过SplitInput 输入分片 决定map的个数在用Record记录 key value。然后分为以下三个流程：
    Map
      输入  key（long类型偏移量）  value（Text一行字符串）
      输出  key value

    Shuffle ...

    Reduce
      输入key时map输出时的key value是分组器分的iterable
      输出 key value
      输出结果保存在hdfs上而不是本地文件中

4.了解zookeeper吗？介绍一下它，它的选举机制和集群的搭建。
    ZooKeeper 是一个开源的分布式协调服务，是 Google Chubby 的开源实现。分布式应用程序可以基于 ZooKeeper 实现
    诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。
    我们公司使用的flume集群，Kafka集群等等，都离不开ZooKeeper呀。每个节点上我们都要搭建ZooKeeper服务。首先我们
    要在每台pc上配置zookeeper环境变量，在cd到zookeeper下的conf文件夹下在zoo_simjle.cfg文件中添加datadir路径，
    再到zookeeper下新建data文件夹，创建myid，在文件里添加上server的ip地址。在启动zkserver.sh start便ok了。

5.mysql，mongodb，rides的端口。

  面试数据库介绍的再好，不知道默认端口，也证明你没有经验。mysql：3306，mongdb：27017，rides：6379。

6.spark streming在实时处理时会发生什么故障，如何停止，解决?
  和Kafka整合时消息无序：

  修改Kafka的ack参数，当ack=1时，master确认收到消息就算投递成功。ack=0时，不需要收到消息便算成功，高效不准确。
  sck=all，master和server都要受到消息才算成功，准确不高效。

  StreamingContext.stop会把关联的SparkContext对象也停止，如果不想把SparkContext对象也停止的话可以把StreamingContext.stop
  的可选参数stopSparkContext设为flase。一个SparkContext对象可以和多个streamingcontext对象关联。只要对前一个stremingcontext.stop
  (stopsparkcontext=false),然后再创建新的stremingcontext对象就可以了。

7.说一下你对hadoop生态圈的认识。

  没有固定答案，主要从hdfs底层存储，hbase数据库，hive数据仓库，flume收集，Kafka缓存，zookeeper分布式协调服务，spark大数据分析，
  sqoop数据互转来说。

8.数据来源的方式：
  1.webServer ：用户访问我们的网站，对日志进行收集，记录在反向的日志文件里 tomcat下logs
  2js代码嵌入前端页面（埋点）：js的sdk会获取用户行为，document会得到元素调用function，通过ngix集群进行日志收集。

9.yarn的理解：

  YARN是Hadoop2.0版本引进的资源管理系统，直接从MR1演化而来。
  核心思想：将MR1中的JobTracker的资源管理和作业调度两个功能分开，分别由ResourceManager和ApplicationMaster进程实现。

  ResourceManager：负责整个集群的资源管理和调度 ApplicationMaster：负责应用程序相关事务，比如任务调度、任务监控和容错等。
  YARN的出现，使得多个计算框架可以运行在同一个集群之中。
  1. 每一个应用程序对应一个ApplicationMaster。
  2. 目前可以支持多种计算框架运行在YARN上面，比如MapReduce、storm、Spark、Flink。

10..聊聊你的项目。
   这是60%的时间都在这，你一定要把自己的项目了解搞清楚，数据的来源，数据的收集，数据的分析，数据的储存，数据的展示。

知识点:
    nagios是集群监控工具，而且是云计算三大利器之一

    如果namenode意外终止，secondarynamenode会接替他是集群继续工作。（错误）
    secondarynamenode是帮助恢复，而不是替代，如何恢复，可以查看hadoop根据secondarynamenode恢复namenode。
    在高可用集群中，一个namenode（active）死亡后，ZKFC（zookeeper控制器）仲裁将另一个standby-namenode启动，
    转换成active状态，集群继续正常工作。

    map 槽---->map slot。（org.apache.hadoop.mapred.TaskTracker.TaskLaucher.numFreeSlots）是一个逻辑值，
    而不是对应着一个线程或者进程。

    hadoop 为各个守护进程（namenode,secondarynamenode,resourcemanager,datanode,nodemanager）统一分配的内存在
    hadoop-env.sh 中设置，参数为 HADOOP_HEAPSIZE，默认为 1000M。

阿里.网易海康:
    (1)spark运行流程、源码架构

    (2)Hbase主键设计、hbase为啥比mysql快、为什么项目选用hbase

    (3)Hbase读写流程，数据compact流程
        读流程：
            1 连接zookeeper，找到meta表所在的regionserver的地址
            2 访问对应的regionserver，读meta表的信息
            3 通过命令找到rowkey对应的region，得到region信息
            4 访问region所在的regionserver
            5 访问对应store 读内存（memstore cache） storefile

         写流程：
            1 连接zookeeper，找到meta表所在的regionserver的地址
            2 访问对应的regionserver，读meta表的信息
            3 通过命令找到rowkey对应的region，得到region信息
            4 访问region所在的regionserver
            5 正常情况
                WAL（hlog） write ahead log
                memstore
                flush：将内存中的数据进行溢写到磁盘变成storefile

        compact:将多个storefile进行合并，变成一个大文件，等到多个store中的storefile的和达到一定阈值，然后进行split
        （region的等分成两个region，由HMaster分配，当前region消失）

    (4)Hadoop mapreduce流程

    (5)Spark standalone模型、yarn架构模型(画出来架构图)


    (6)Spark算子(map、flatmap、reducebykey和reduce、groupbykey和reducebykey、join、distinct)原理

    (7)Spark stage的切分、task资源分配、任务调度、master计算资源分配

    (8)Sparksql自定义函数、怎么创建dateframe

    (9)SparkStreaming项目多久一个批次数据

    (10)Kafka复制机制、分区多副本机制

    (11)Hdfs读写流程，数据checkpoint流程

    (12)Sparkshuffle和hadoopshuffle原理、对比

    (13)Hivesql怎么转化为MapReduce任务

    (14)Spark调优

    (15)Spark数据倾斜解决方案

    (16)Yarn工作流程、组成架构

    (17)Zookeeper首领选取、节点类型、zookeeper实现原理

    (18)hbase的ha，zookeeper在其中的作用

    (19)spark的内存管理机制，spark1.6前后对比分析

    (21)spark rdd、dataframe、dataset区别

    (22)spark里面有哪些参数可以设置，有什么用

    (23)hashPartitioner与rangePartitioner的实现

    (24)spark有哪几种join

    (25)spark jdbc(mysql)读取并发度优化

    (26)Spark join算子可以用什么替代

    (27)HBase region切分后数据是怎么分的

    (28)项目集群结构(spark和hadoop集群)

    (29)spark streaming是怎么跟kafka交互的，具体代码怎么写的，程序执行流程是怎样的，这个过程中怎么确保数据不丢(直连和receiver方式)

    (30)kafka如何保证高吞吐的，kafka零拷贝，具体怎么做的

    (31)hdfs的容错机制

    (32)zookeeper怎么保证原子性，怎么实现分布式锁

    (33)kafka存储模型与网络模型

    (34)Zookeeper脑裂问题
        https://blog.csdn.net/ma_shou_feng/article/details/84898305#2__ZooKeeper_15

Scala

  (1)隐式转换(implicit)
     object Implicit{
        def main(args:Array[String]):Unit={
            implicit val a:Int=10
            def m2(a:Int)(implicit b:Int)=a+b
            print(m2(5)(4)) //9
            print(m2(5))  //15
        }
     }
    隐式转换的注意事项：
    ①隐式参数值是从上下文环境中寻找，匹配使用implicit修饰的变量，且最多只有一个，与方法参数列表中隐式类型一致，并使用。
    ②如果匹配到多个，报错。
    ③如果上下文中没有，则使用方法参数列表中的默认隐式值。
    ④如果没有默认值，报错。
    ⑤一个参数列表中只能有一个implicit关键字，implicit放到最后一个列表中，并修饰该列表中的所有参数，如：
    ---------------------
    def m2(a:Int)(b:Int)(implicit c:Int,d:Int)
    implicit val scala:Int=20
    def m3(a:Int)(b:Int)(implicit c:Int,d:Int)=a+b+c+d
    println(m3(5)(5))//结果为 50=5+5+20+20


  (2)柯理化
     scala中的curring是将一个正常的方法转换为柯里化的一个过程
     把一个参数列表中的多个参数转换为多个列表
     def m1(a:Int,b:Int)=a+b
     def m2(a:Int)(b:Int)=a+b


  (3)scala中_的用法
       1.方法转化为函数
            def m1(x:Int,y:Int)=x*y
            val f1 = m1 _
       2.集合中的每一个元素
            val list = List(1,2,3)
            val list1 = list.map(_ * 10)
       3.获取元组tuple中的元素
            val t = ("hadoop",3.14,100)
                t._1  t._2 ...
       4.模式匹配
            val word = "spark"
            val result = word match{
                case "hadoop" => 1
                case "spark" => 2
                case _ => 0
            }
       5.队列
            val list = List(1,2,3,4)
            list match{
                case List(_,_*) => 1
                case _ => 2
            }
       6.导包引入的时候
            import scala.collection.mutable._
            表示将该包下面所有的类都导入
       7.初始化变量
            val name:String = _  等价于 val name：String = null
            val age:Int = _     等价于 val age:Int = 0

    https://blog.csdn.net/qq_41455420/article/details/79440164