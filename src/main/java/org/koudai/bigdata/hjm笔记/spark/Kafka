readme:
1.Kafka是什么?(了解)
    在流式计算中,kafka一般用来缓存数据,Storm通过消费Kafka的数据进行计算
    KAFKA+STORM+REDIS
    Kafka是一个开源消息系统,
         由Scala写成.目标是为处理实时数据提供一个统一.高吞吐量.低等待的平台.
    Kafka是一个分布式消息队列:
          生产者消费者的功能.它提供了类似于JMS的特性,
          但是在设计上完全不同,此外它并不是JMS规范的实现.
          Kafka需要依赖于zookeeper集群保存一些meta信息,保证系统可用性

注意:kafka集群不包括producer和consumer!
    kafka只有数据存储的功能,没有数据拉取和数据发送的功能.

2.Kafka设计：重要
    1.高吞吐量
        1.数据磁盘持久化：消息不在内存中cache，直接写入到磁盘，充分利用磁盘的顺序读写功能。
        2.zero-copy：减少IO操作步骤
        3.数据批量发送
        4.数据压缩
        5.Topic划分为多个partition，提高parallelism
    2.负载均衡
        1.producer通过用户指定的算法，将消息发送到指定的partition。
        2.存在多个partition，每个partition有自己的replica，每个replica分布在不同的Broker节点上。
        3.多个partition需要选取出lead partition，lead partition负责读写，并由zookeeper负责fail over。
        4.通过zookeeper管理broker与consumer的动态加入与离开。



2.JMS是什么?(了解)
    JMS是java提供的一套技术规范
    作用:异构系统 集成通信 缓解系统瓶颈...
    JMS消息传输模型:点对点模式,发布订阅模式(一对多)
    JMS核心组件:Destination Message Producer MessageConsumer
    常见的类JMS消息服务器:
        ActiveMQ
    分布式消息中间件:
        Metamorphosis 简称:MetaQ  --高性能,高可用,可扩展
        3.0版本:RocketMQ

Nosql数据库:非关系型数据库 https://baike.baidu.com/item/NoSQL/8828247?fr=aladdin

3.为什么需要消息队列?(重要,了解)
消息系统的核心作用就是三点：解耦，异步和并行

4.Kafka核心组件(重要)!!!
    Topic:消息根据Topic进行归类,可以理解为一个队列,里面有分区的概念
    Producer:发送消息者
        --生产者负责采集数据并把数据传输到Kafka的某个topic中.比如:flume.java后台服务.shell脚本.logstash
        --生产者是由多个进程组成的(可以有多个生产者组成),一个生产者可以作为一个独立的进程,可以独立的分发数据.
        --多个生产者发送的数据可以存储到同一个topic的同一个partition
        --一个生产者的数据可以放到多个topic中
    Consumer:消息接收者,有group组的概念,无集群概念
        --负责消费数据:flume,sparkStreaming,Storm...
        --一个ConsumerGroup也被称为Consumer集群(不严谨)
        --新增或减少Consumer的数量会触发Kafka的负载均衡
        --ConsumerGroup可以消费一个或多个分区的数据,相反,一个或多个分区的数据同时只能被一个Consumer消费
        --ConsumerGroup成员之间消费的数据各不相同,在同一个group中数据是不可以被重复消费的(一次仅一个语义)

    broker:每个kafka实例(server)也叫每一个节点
    zookeeper:依赖集群保存meta信息

多个producer可以向topic同一个分区里写数据
多个consumer不可以同时获取一个topic分区里的数据,会出现线程等待
一次仅一次语义 ---把组名改了可以再次拉取数据
当消费者的数量发生改变时,会触发kafka集群的负载均衡(kafka自己触发)

kafka集群节点个数为奇数个,存在选举机制

Kafka集群:
    --Kafka集群可以保存多种数据类型的数据,一种数据类型可以存放到一个topic中.
    --每个topic可以创建一个或多个分区,分区的数量和副本的数量是在创建topic时指定的.
      在后期,某个topic的分区数可以重新指定(只能由少到多)
    --每个分区是由多个segment组成,segment的大小是可以配置的,默认是1G.
      segment里面有两种类型的文件(index和log文件),index文件存放log数据对应的索引,log文件存放数据.
    --Kafka是具有多副本机制的,原始数据和副本数据是不可以在同一个节点中的.


5.Kafka集群部署
集群搭建
修改配置文件:
   F:\spark\sparkcourse软件\kafka\conf
   三个配置文件: 把之前的三个删除或备份,将三个直接拷贝进去,然后修改
    1.producer.properties
    2.consumer.properties
    3.sever.properties

6.Kafka不是完全同步,也不是完全异步,是一种ISR机制:
  1 leader会维护一个与其基本保持同步的Replica列表,该列表称为ISR(in-sync Replica:同步副本),
    每个Partition都会有一个ISR,而且是由leader动态维护
  2 如果一个flower比一个leader落后太多,或者超过一定时间未发起数据复制请求,则leader将其从ISR中移除
  3 当ISR中所有Replica都向Leader发送ACK时,leader才commit

7.既然所有Replica都向Leader发送ACK时,leader才commit,那么flower怎么会比leader落后太多?
    producer往kafka中发送数据，不仅可以一次发送一条数据，还可以发送message的数组；批量发送，
    同步的时候批量发送，异步的时候本身就是就是批量；底层会有队列缓存起来，批量发送，对应broker而言，
    就会收到很多数据(假设1000)，这时候leader发现自己有1000条数据，flower只有500条数据，落后了500条数据，
    就把它从ISR中移除出去，这时候发现其他的flower与他的差距都很小，就等待；如果因为内存等原因，差距很大，
    就把它从ISR中移除出去。

8.Kafka的几个问题:
  1 每个topic的分区中有多个segment,一个分区会被分成相同大小数据数量不等的segment,
     数据的生命周期就是指的segment的生命周期.
  2 数据的存储机制
       首先是Broker接收到数据,将数据放到操作系统的缓存里(pagecache),
       pagecache会尽可能多的使用空闲内存
       使用sendfile技术尽可能多的减少操作系统和应用程序之间的重复缓存
       写数据的时候使用的是顺序写入,顺序写入的速度可达600m/s

  3 kafka是怎么解决负载均衡的呢?
        首先获取Consumer消费的起始分区号
        然后计算出Consumer要消费的分区数量
        用起始分区号的hash值%分区数

  4 数据是怎么分发的?
        kafka默认调用自己的分区器(DefaultPartitioner),
        当然也可以用自定义分区器,需要实现Partitioner特质,实现partitioner方法

  5 怎么保证存储的数据不丢失(安全性)
    kafka的多副本机制就保证了数据的不丢失,副本数是在创建topic时指定的.

  6 streaming xm  总吞吐量
    Topic ym  一个分区的吞吐量
    xm / ym = partition 分区数量
    分区的数量最好是consumer数量的整数倍(2~3)

   7 如何做到topic全局分区数据有序(卡夫卡的性质是分区数据无序,分区内有序!)
     方法一:声明为一个分区,这样会影响吞吐量
     方法二:只能在消费完成之后手动排序

kafka常见问题 见文档:


实现一个生产者:
    ProducerDemo

def main (args:Array[String]):Unit = {
    val topic = "test"
    val props = new Properties()
    props.put("serializer.class","kafka.serializer.StringEncoder")
    props.put("metadata.broker.list","min1:9092,min2:9092")
    ...
    props.put("request.required.acks","0")//不严格给0 严格1 非常严格-1
    props.put("partitioner.class","kafka.producer.DefaultPartitioner")

    val config = new ProducerConfig(props)
    val producer:[String,String] = new Producer(config) //得到生产者实例
    for(i<- 1 to 10000){
     val msg = s"$i :Producer send data....."
     producer.send(new KeyedMessage[String,String](topic,msg))
    }
  }

实现一个消费者:
    ConsumerDemo
class ConsumerDemo(val consumer:String,val stream:KafkaStream[Array[Byte],Array[Byte]])
extends Runnable{
    override def run() = {
     val it = stream.iterator()
    while (it.hasNext()) {
      val data = it.next()
      val offset = data.offset
      val partition = data.partition
      val topic = data.topic
      val msg = new String(data.message())

      println(s"Consumer:$consumer,Topic:$topic,Partition:$partition,offset:$offset,msg:$msg")
    }

    }
}
object ConsumerDemo{
    def main(args:Array[String]):Unit ={
        val topic = "test"
        val topics = new mutable.HashMap[String,Int]()
        topics.put(topic,2)
        val props = new Properties()
        props.put("group.id","group1)
        props.put("zookeeper.connect","min1:2181,min2:2181,min3:2181")....

        val config = new ConsumerConfig(props)
        val consumer = Consumer.create(config)
        val streams = consumer.createMessageStreams(topics)

        val stream = streams.get(topic)
        val pool = Executors.newFixedThreadPool(300)
        for(i<- 0 util stream.size){
        pool.execute(new ConsumerDemo(s"consumer:$i",stream.get(i)))
        }

    }
}


7、SparkStreaming之Kafka的Receiver和Direct方式讲解
7.1、Receiver方式
Receiver是使用Kafka的high level的consumer API来实现的。Receiver从Kafka中获取数据都是存储在Spark Executor内存中的，
然后Spark Streaming启动的job会去处理那些数据

然而这种方式很可能会丢失数据，如果要启用高可靠机制，让数据零丢失，就必须启动Spark Streaming预写日志机制。该机制会同步
地接收到Kafka数据写入分布式文件系统，比如HDFS上的预写日志中。所以底层节点出现了失败，也可以使用预写日志的数据进行恢复
7.2、Direct方式
它会周期性的查询kafka，来获取每个topic + partition的最新offset，从而定义每一个batch的offset的范围。当处理数据的job启动时，
就会使用kafka简单的消费者API来获取kafka指定offset的范围的数据。

1）它简化了并行读取：如果要读取多个partition，不需要创建多个输入DStream然后对他们进行union操作。
Spark会创建跟kafka partition一样多的RDD partition，并且会并行从kafka中读取数据。所以在kafka partition
和RDD partition之间有一个一一对应的映射关系。

2）高性能：如果要保证数据零丢失，基于Receiver的机制需要开启WAL机制，这种方式其实很低效，因为数据实际上被
copy了2分，kafka自己本身就有可靠的机制，会对数据复制一份，而这里又复制一份到WAL中。基于Direct的方式，不
依赖于Receiver，不需要开启WAL机制,只要kafka中做了数据的复制，那么就可以通过kafka的副本进行恢复。

3）一次仅且一次的事务机制
基于Receiver的方式，是使用Kafka High Level的API在zookeeper中保存消费过的offset的。这是消费kafka数据的传统
方式，这种方式配合这WAL机制可以保证数据零丢失，但是无法保证数据只被处理一次的且仅且一次，可能会两次或者更多，
因为spark和zookeeper可能是不同步的。

4）降低资源
Direct不需要Receivers，其申请的Executors全部参与到计算任务中；而Receiver-based则需要专门的Receivers来读取
Kafka数据且不参与计算。因此相同的资源申请，Direct 能够支持更大的业务。

5）降低内存
Receiver-based的Receiver与其他Exectuor是异步的，并持续不断接收数据，对于小业务量的场景还好，如果遇到大业务
量时，需要提高Receiver的内存，但是参与计算的Executor并无需那么多的内存。而Direct 因为没有Receiver，而是在计
算时读取数据，然后直接计算，所以对内存的要求很低。实际应用中我们可以把原先的10G降至现在的2-4G左右。

6）不会出现数据堆积
Receiver-based方法需要Receivers来异步持续不断的读取数据，因此遇到网络、存储负载等因素，导致实时任务出现堆积，
但Receivers却还在持续读取数据，此种情况很容易导致计算崩溃。Direct 则没有这种顾虑，其Driver在触发batch 计算任
务时，才会读取数据并计算。队列出现堆积并不会引起程序的失败。

基于receiver的方式，是使用Kafka的高阶API来在ZooKeeper中保存消费过的offset的。这是消费Kafka数据的传统方式。
这种方式配合着WAL机制可以保证数据零丢失的高可靠性，但是却无法保证数据被处理一次且仅一次，可能会处理两次。
因为Spark和ZooKeeper之间可能是不同步的。

基于direct的方式，使用kafka的简单api，Spark Streaming自己就负责追踪消费的offset，并保存在checkpoint中。Spark
自己一定是同步的，因此可以保证数据是消费一次且仅消费一次。



kafka到spark streaming怎么保证数据完整性，怎么保证数据不重复消费？
    保证数据不丢失（at-least）
         spark RDD内部机制可以保证数据at-least语义。
    Receiver方式
         开启WAL（预写日志），将从kafka中接受到的数据写入到日志文件中，所有数据从失败中可恢复。
    Direct方式
          依靠checkpoint机制来保证。
          保证数据不重复（exactly-once）
         要保证数据不重复，即Exactly once语义。
    - 幂等操作：重复执行不会产生问题，不需要做额外的工作即可保证数据不重复。
    - 业务代码添加事务操作
        就是说针对每个partition的数据，产生一个uniqueId，只有这个partition的所有数据被完全消费，则算成功，
        否则算失效，要回滚。下次重复执行这个uniqueId时，如果已经被执行成功，则skip掉。


Spark-Streaming获取kafka数据的两种方式,优缺点?
    1.Receiver方式:当一个任务从driver发送到Executor执行的时候,将数据拉取到Executor中操作,
但是如果数据太大的话,就不能全放在内存中,这时候,Receiver通过WAL(Write-Ahead Logging:预写日志系统)
设置了本地存储,他会存放本地,保证数据不丢失,然后使用kafka高级API通过zk来维护偏移量,保证数据的衔接性,
其实可以说,receiver数据在zk获取的,这种方式效率低,而且极容易出现数据丢失.
    2.Direct方式:使用Kafka底层API并且消费者直接连接在Kafka的分区上,因为createDirectStream创建的
DirectKafkaInputDStream每个batch(批次)对应的RDD的分区与Kafka的分区一一对应,但是需要自己维护偏移量,
迭代计算,即用即取即丢,不会给内存造成太大的压力,效率很高.

Kafka消息不能重复消费？
同一topic的一条消息只能被同一个consumer group内的一个consumer消费。

创建Kafka分区：
·bin/kafka-topics.sh --create --zookeeper min1:2181 --replication-factor 1 --partitions 1 --topic test


Kafka性能调优：






1、kafka在高并发的情况下，如何避免消息丢失和消息重复
   消息丢失解决方案:
       首先对kafka进行限速,其次启用重试机制,重试间隔时间设置长一些,最后Kafka设置acks=all,
   即需要相应的所有处于ISR的分区都确认收到该消息后,才算发送成功.

   消息重复解决方案:
        消息可以使用唯一标识
        生产者(ack=all 代表至少成功发送一次)
        消费者(offset手动提交,业务逻辑成功处理后,提交offset)
        落表(主键或者唯一索引的方式,避免重复数据)
        业务逻辑处理(选择唯一主键存储到Redis或者mongodb中,先查询是否存在,若存在则不处理,
                     若不存在,先插入Redis或MongoDB,再进行业务逻辑处理)

2、Kafka怎么保证数据消费一次且仅消费一次
    幂等producer:保证发送单个分区的消息只会发送一次,不会出现重复消息.
    事务(transaction):保证原子性的写入到多个分区,即写入到多个分区的消息要么全部成功,要么全部回滚
    流处理EOS:流处理本质上可以看成是"读取-处理-写入"的管道,此EOS保证整个过程的操作是原子性.注意,
    这只适用于Kafka Streams

3、Kafka保证数据一致性和可靠性
    数据一致性保证
    一致性定义:若某条消息对client可见,那么即使Leader挂了,在新Leader上数据依然可以被读到
    HW-HighWaterMark:client可以从Leader读到最大msg offset,即对外可见的最大offset,HW=max(replica.offset)
    对于Leader新收到的msg,client不能立刻消费,Leader会等待该消息被所有ISR中的replica同步后,更新HW,此时该
    消息才能被client消费,这样就保证了如果leader fail,该消息仍然可以从新选举的leader中获取.
    对于来自内部broker的读取请求,没有HW的限制.同时,Follower也会维护一份自己的HW,Follower.HW = min(Leader.HW, Follower.offset)
    数据可靠性保证:
    当Producer向Leader发送数据时,可以通过acks参数设置数据可靠性的级别
    0:无论是否写入成功,server不需要给Producer发送Response,如果发生异常,server会终止连接,触发Producer更新meta数据;
    1:leader写入成功后即发送Response,此种情况如果leader fail,会丢失数据.
    -1:等待所有ISR接收到消息后再给Producer发送Response,这是最强保证.

4、Kafka到spark streaming怎么保证数据完整性，怎么保证数据不重复消费?
    保证数据不丢失(at least once)
        spark RDD内部机制可以保证数据at least once 语义.
    Receiver方式
        开启WAL(预写日志),将从kafka中接收到的数据写入到日志文件中,所有数据从失败中可恢复
    Direct方式
        依靠checkpoint机制来保证

    保证数据不重复(exactly once)
        要保证数据不重复,即Exactly once语义.
        -幂等操作:重复执行不会产生问题,不需要做额外的工作即可保证数据不重复.
        -业务代码添加事务操作
        就是说针对每个partition的数据,产生一个uniqueId,只有这个partition的所有数据被完全
        消费,则算成功,否则算失效,要回滚.下次重复执行这个uniqueId时,如果已经被执行成功,则skip掉

5、kafka的消费者高阶和低阶API有什么区别
    kafka提供了两套consumer API:The high-level Consumer API 和The SimpleConsumer API
    其中high-level consumer API提供了一个从kafka消费数据的高层抽象,而SimpleConsumer API
    则需要开发人员更多的关注细节.

    The high-level consumer API
    high-level consumer API 提供了consumer group 的语义,一个消息只能被group内的一个consumer
    所消费,且consumer消费消息时不关注offset,最后一个offset由zookeeper保存.
    使用high-level consumer API 可以是多线程的应用,应当注意:
    如果消费者线程大于partition数量,则有些线程将收不到消息.
    如果partition数量大于线程数,则有些线程收到多个partition的消息
    如果一个线程消费多个partition,则无法保证收到的消息的顺序,而一个partition内的消息是有序的

    The SimpleConsumer API
    如果想要对partition有更多的控制权,那就用The SimpleConsumer API.
    比如:
    多次读取一个消息
    只消费一个partition中的部分消息
    使用事务来保证一个消息仅被消费一次但是使用此API时,partition,offset,broker,leader等对你不在
    透明,需要自己去管理.你需要做大量的额外工作:必须在应用程序中跟踪offset,从而确定下一条应该消费
    哪条消息.
    应用程序需要通过程序获知每个Partition的leader是谁
    需要处理leader的变更

扩展:
kafka的exactly-once
    幂等producer：保证发送单个分区的消息只会发送一次，不会出现重复消息
    事务(transaction)：保证原子性地写入到多个分区，即写入到多个分区的消息要么全部成功，要么全部回滚
    流处理EOS：流处理本质上可看成是“读取-处理-写入”的管道。此EOS保证整个过程的操作是原子性。
        注意，这只适用于Kafka Streams

如何保证从Kafka获取数据不丢失?

    1.生产者数据的不丢失
    kafka的ack机制：在kafka发送数据的时候，每次发送消息都会有一个确认反馈机制，确保消息正常的能够被收到。
    2.消费者数据的不丢失
    通过offset commit 来保证数据的不丢失，kafka自己记录了每次消费的offset数值，下次继续消费的时候，
    接着上次的offset进行消费即可。






