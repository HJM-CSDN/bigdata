首先什么是hive:
    Hive是一个基于Hadoop的数据仓库平台。通过hive，我们可以方便地进行ETL的工作。
    hive定义了一个类似于SQL的查询语言：HQL，能 够将用户编写的QL转化为相应的Mapreduce程序基于Hadoop执行。
    Hive 可以看成是从SQL到Map-Reduce的 映射器

1.内部表,外部表,相同与区别
  未被external修饰的是内部表(managed table),被external修饰的为外部表
  区别:
     1 内部表数据由hive自身管理,外部表数据由HDFS管理;
     2 内部表数据存储的位置是hive.metastore.warehouse.dir(默认/user/hive/warehouse),
       外部表数据的存储位置由自己指定
     3 删除内部表会直接删除元数据(metadata)及存储位置;删除外部表仅仅会删除
       元数据,HDFS上的文件并不会被删除;
     4 对内部表的修改会将修改直接同步给元数据,而对外部表的表结构和分区进行修改,
       则需要修复（MSCK REPAIR TABLE table_name）

2.分区表技术与意义
    将表内的数据按照业务纬度列(常见的有:时间,地域,类别)进行进一步细分
    1.可以减少数据冗余
    2.提高特定(指定分区)查询分析的效率

分区表DDL操作:
    创建一个分区表:
        create table test.birth(id string,name string) partitioned by (birthdate string)
          row format delimited fields terminated by ',';
    查询分区表中已有的分区:
        show partitions table_name;
    在给定的分区表中添加分区:
        alter table test.birth add partition(birthdate='1990-01-01');
分区表DML操作:
    1 静态分区数据插入:load
      load data local inpath '/root/birth1990-01-01' into table test.birth
      Partition(birthdate='1990-01-01');
    2 查看hive某一配置的当前值:
      set 配置项名字
    如查看当前版本的hive中动态分区是否开启
    set hive.exec.dynamic.partition;
    3 动态往test.birthpartition分区表中添加数据
      1 设置hive动态分区模式为非严格模式
        set hive.exec.dynamic.partition.mode=nonstrict;
      2 动态分区之前,一般会有较大历史数据集,里面包含多个分区的数据
        文件:birthhistory
        id,birthday,name
        1,1990-12-12,hujie
        2,1990-12-12,wanglujie
        3,1990-12-13,yanghanjie
        4,1990-12-14,yueshengdama
        5,1990-11-10,shenkaidama
      3 创建一个外部表，用来描述历史数据集的结构
        create table test.birthhistory(
        id int,
        birthdate string,
        name string
        )
        row format delimited fields terminated by ',';
      4 创建一个分区表，用来分区盛放历史数据集中的所有数据
        create table test.birthpartition(
        id string,
        name string
       )
      partitioned by (birthdate string)
      row format delimited fields terminated by ',';
      5 从上述创建的test.birthhistory表中查询所有数据，动态插入到test.birthpartition表中
        from test.birthhistory
        insert into table test.birthpartition partition(birthdate)
        select id,name,birthdate;
        注意：分区列应该在查询的所有列的最后的位置

3.分桶表 分区与意义
    hive中分区的本质:在hdfs上建立目录,对数据进行更细粒度的划分
    与分区表类似,hive表中,分区中均可以指定列,然后对其分桶.分桶表是对
    列值取哈希值的方式,将不同数据放到不同文件中存储,对于hive中每一个
    表,分区都可以进一步进行分桶.分桶列与分区列使用的列一般不同.
  分桶表与分区表区别
   分区表根据指定的列将表进行分区，列值相同的数据会分到同一个分区。
   分桶表一般是在分区表的基础上，对其他业务列进行分桶。
    分桶表优点
    （1）获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。
        具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，
        可以使用 Map 端连接 （Map-side join）高效的实现。比如JOIN操作。
        对于JOIN操作两个表有一个相同的列，如果对这两个表都进行了桶操作。
        那么将保存相同列值的桶进行JOIN操作就可以，可以大大较少JOIN的数据量。
     例如：雇员表和部门表两个表，指定雇员表中的所属部门id列为分桶列，指定部门表中的id为分桶列。

    （2）使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，
        如果能在数据集的一小部分数据上试运行查询，会带来很多方便。

4.窗口函数 (实现级联求和？）
   https://blog.csdn.net/qq_34941023/article/details/52590176
  LEAD  与后一行
  可以选择指定要引导的行数。如果未指定要引导的行数，则前导是一行。
  当当前行的前导超出窗口末尾时返回null。
  LAG  与前一行
  可以选择指定滞后的行数。如果未指定滞后行数，则滞后为一行。
  当当前行的延迟在窗口开始之前延伸时，返回null。

  lag 和lead 有三个参数，第一个参数是列名，第二个参数是偏移的offset，第三个参数是 超出记录窗口时的默认值）

  FIRST_VALUE
  这最多需要两个参数。第一个参数是您想要第一个值的列，第二个（可选）参数必须是false默认的布尔值。
  如果设置为true，则跳过空值。
  LAST_VALUE
  这最多需要两个参数。第一个参数是您想要最后一个值的列，第二个（可选）参数必须是false默认的布尔值。
  如果设置为true，则跳过空值

SQL> select *  from kkk;

        ID NAME
---------- --------------------
         1 1name
         2 2name
         3 3name
         4 4name
         5 5name

SQL> select id,name,lag(name,1,0) over ( order by id )  from kkk;

        ID NAME                 LAG(NAME,1,0)OVER(ORDERBYID)
---------- -------------------- ----------------------------
         1 1name                0
         2 2name                1name
         3 3name                2name
         4 4name                3name
         5 5name                4name

  需求
  将部门内员工的薪资数据按照从高到低排序，然后逐人查看与上一个人的薪资差异
  Select xxx,(lag(salary,1,0) over(partition by dep_name order by salary desc)-salary)as difference
    from test.t_employee;

  数据
  使用上述分析函数中的部门薪资表

  实现
  SELECT
      dep_name, emp_name, salary,
      (LAG(salary,1,0) OVER (PARTITION BY dep_name ORDER BY salary DESC)-salary) AS salary_difference
  FROM test.t_employee
  ;

  ------扩展，使用FIRST_VALUE取部门内最高薪资（注意会返回多个值）
  --查询部门内薪资最高的员工的详情 by FIRST_VALUE
  SELECT
      dep_name, emp_name,
      FIRST_VALUE(salary) OVER (PARTITION BY dep_name ORDER BY salary DESC) AS maxsalary
  FROM test.t_employee

5.排名函数,给出案例,实现
    示例
    工资表查询部门内最高薪资
    --创建表
    CREATE table test.t_employee (
      id INT,
      emp_name VARCHAR(20),
      dep_name VARCHAR(20),
    salary DECIMAL(7,2),
    age DECIMAL(3,0)
    );
    --加载数据
    INSERT INTO test.t_employee VALUES
    ( 1,'Matthew','Management',4500,55),
    ( 2,'Olivia','Management',4400,61),
    ( 3,'Grace','Management',4000,42),
    ( 4,'Jim','Production',3700,35),
    ( 5,'Alice','Production',3500,24),
    ( 6,'Michael','Production',3600,28),
    ( 7,'Tom','Production',3800,35),
    ( 8,'Kevin','Production',4000,52),
    ( 9,'Elvis','Service',4100,40),
    (10,'Sophia','Sales',4300,36),
    (11,'Samantha','Sales',4100,38);

    --查询所有部门中薪资最高的员工的详情 by rank（最高薪资有多人的，全部取出）
    SELECT dep_name, emp_name, salary
    FROM (
      SELECT
        dep_name, emp_name, salary
        ,RANK() OVER (PARTITION BY dep_name ORDER BY salary DESC) AS rnk
      FROM test.t_employee
    ) as a --hive子查询如果作为被查询的对象，必须取别名
    where rnk = 1;

    ------以下示例，演示rank和rownumber的区别-------
    --单独插入一条数据，人为制造某一部门内的人员薪酬相同
    INSERT INTO test.t_employee VALUES
    (100,'huge','Sales',4300,36);

    --查询所有部门中薪资最高的员工的详情 by row_number() (最高薪资有多人的，只取其中一个)
    SELECT dep_name, emp_name, salary
    FROM (
      SELECT
        dep_name, emp_name, salary
        ,ROW_NUMBER() OVER (PARTITION BY dep_name ORDER BY salary DESC) AS rnk
      FROM test.t_employee
    ) as a --hive子查询如果作为被查询的对象，必须取别名
    where rnk = 1;

6.distribute by,partition by 与 cluster by 的区别
    distribute by是控制在map端如何拆分数据给reduce端的。hive会根据distribute by后面列，对应reduce的个数进行分发，
    默认是采用hash算法。sort by为每个reduce产生一个排序文件。在有些情况下，你需要控制某个特定行应该到哪个reducer，
    这通常是为了进行后续的聚集操作。distribute by刚好可以做这件事。因此，distribute by经常和sort by配合使用。
    注：Distribute by和sort by的使用场景
    1.Map输出的文件大小不均。
    2.Reduce输出文件大小不均。
    3.小文件过多。
    4.文件超大。

    用distribute by 会对指定的字段按照hashCode值对reduce的个数取模，然后将任务分配到对应的reduce中去执行
    就是在mapreduce程序中的partition分区过程，默认根据指定key.hashCode()&Integer.MAX_VALUE%numReduce 确定处理该任务的reduce
    distribute by 和 sort by 合用就相当于cluster by，但是cluster by 不能指定排序为asc或 desc 的规则，只能是desc倒序排列。

7.sort by与 order by的区别
    order by 单维度全量排序，所有数据会汇聚到一个ReduceTask上，效率不高，
    一般用order by + limit 取单维度的TOPN。
    用distribute by + sort by组合将相同值（例如：成绩的科目）的数据发送到同一个Reduce上并进行排序。

8.hive 优化
    1.环境优化(linux句柄数,应用内存分配,是否负载等)
    2.应用配置属性方面的优化
    3.代码优化(hql,尝试换一种hql的写法)

    1.学会看explain
     explain:显示hql查询的计划,explain extended:显示hql查询的计划,还会显示hql的抽象表达式树
    2.对limit的优化
        hive.limit.row.max.size=100000
        hive.limit.optimize.limit.file=10
        hive.limit.optimize.enable=false
    3.对join的优化
        小表驱动大表
        实际上是重复关联键少的表放在join的前面.
    4.使用hive本地模式(在一个jvm里面运行)
        hive.exec.mode.local.auto=false(默认为false):设置为true后开启本地模式
        hive.exec.mode.local.auto.inputbytes.max=134217728
        hive.exec.mode.local.auto.input.files.max=4
    5.hive并行执行(stage之间并没有相互依赖关系的可以并行执行)
        hive.exec.parallel=false(默认为false):改为true后,顺序执行变为并发执行
        hive.exec.parallel.thread.number=8
        并发执行适合于资源充足的时候想要提高速度
    6.严格模式
        hive提供的严格模式阻挡三种查询:
        1 带有分区的表的查询
        2 带有order by的查询,除非带有limit语句
        3 join查询语句,不带on条件或者where条件,即不能进行笛卡尔积的查询
    7.设置mapper和reducer个数
        mapper个数太多,启动耗时,个数太少,资源利用不充分.
        reducer个数太多,启动耗时,个数太少,资源利用不充分.
        mapper个数手动设置:set mapred.map.tasks=2;
        通过合并小文件来减少mapper个数
        reducer个数手动设置:set mapreduce.job.reduce=-1;
    8.hive使用jvm重用
        mapreduce.job.jvm.numtasks=1
        set mapred.job.reeuse.jvm.num=8;
    9.数据倾斜

    10.job数量的控制
        连接查询的on中的连接字段类型尽可能相同.通常是一个hql语句生成一个job,有join,limit,group by都将有可能会
        生成一个独立的job.
    分区,分桶,索引 这些本身就是对hive的一种优化.

    •	长期观察hadoop处理数据的过程，有几个显著的特征:
    1.	不怕数据多，就怕数据倾斜。
    2.	对jobs数比较多的作业运行效率相对比较低，比如即使有几百行的表，如果多次关联多次汇总，产生十几个jobs，没半小时是跑不完的。
        map reduce作业初始化的时间是比较长的。
    3.	对sum，count来说，不存在数据倾斜问题。
    4.	对count(distinct ),效率较低，数据量一多，准出问题，如果是多count(distinct)效率更低。
    •	优化可以从几个方面着手：
    1.	好的模型设计事半功倍。
    2.	解决数据倾斜问题。
    3.	减少job数。
    4.	设置合理的map reduce的task数，能有效提升性能。(比如，10w+级别的计算，用160个reduce，那是相当的浪费，1个足够)。
    5.	自己动手写sql解决数据倾斜问题是个不错的选择。
        set hive.groupby.skewindata=true;这是通用的算法优化，但算法优化总是漠视业务，习惯性提供通用的解决方法。
        Etl开发人员更了解业务，更了解数据，所以通过业务逻辑解决倾斜的方法往往更精确，更有效。
    6.	对count(distinct)采取漠视的方法，尤其数据大的时候很容易产生倾斜问题，不抱侥幸心理。自己动手，丰衣足食。
    7.	对小文件进行合并，是行至有效的提高调度效率的方法，假如我们的作业设置合理的文件数，对云梯的整体调度效率也会产生积极的影响。
    　　优化时把握整体，单个作业最优不如整体最优。

9.hive数据仓库概念,建模
    Apache Hive™ 数据仓库软件为分布式存储的大数据集上的读、写、管理提供很大方便，同时还可以用SQL语法在大数据集上查询。
    1、是一种易于对数据实现提取、转换、加载的工具(ETL)的工具。可以理解为数据清洗分析展现。
    2、它有一种将大量格式化数据强加上结构的机制。
    3、它可以分析处理直接存储在hdfs中的数据或者是别的数据存储系统中的数据，如hbase。
    4、查询的执行经由mapreduce完成。
    5、hive可以使用存储过程
    6、通过Apache YARN和Apache Slider实现亚秒级的查询检索。

    Hive建模
    1、介绍
    Hive作为数据仓库，同关系型数据库开发过程类似，都需要先进行建模，所谓建模，就是对表之间指定关系方式。
    建模在hive中大致分为星型、雪花型和星座型。要对建模深入理解，首先需要对hive数仓中的集中表概念进行界定。
    hive中的表从形态上分内部表、外部表、桶表、分区表。在数据逻辑上划分为维度表和事实表。
    维度表等价于我们常说的字典表。事实表就是字典表之外的数据表。
    1.1 星型
    多张维度表，一张事实表，维度表之间没有关系。查询性能要好些,存储有冗余的。星型模型使用的比较多。
    1.2 雪花型
    雪花型是星型建模的扩展，维度表之间有关系。存储减少冗余，查询性能有损失，需要多级连接。和星型模型的共性就是只有一张是事实表。
    1.3 星座型
    星座型也是星型模型的扩展，存在多张事实表。

10.数据倾斜,原因与解决方案
    hive对数据倾斜的优化
    数据倾斜的原因
    1、操作：
    关键词	情形	后果
    Join	其中一个表较小，
    但是key集中	分发到某一个或几个Reduce上的数据远高于平均值
    	大表与大表，但是分桶的判断字段0值或空值过多	这些空值都由一个reduce处理，灰常慢
    group by	group by 维度过小，
    某值的数量过多	处理某值的reduce灰常耗时
    Count Distinct	某特殊值过多	处理此特殊值的reduce耗时
    2、原因：
    1)、key分布不均匀
    2)、业务数据本身的特性
    3)、建表时考虑不周
    4)、某些SQL语句本身就有数据倾斜

    3、表现：
    任务进度长时间维持在99%（或100%），查看任务监控页面，发现只有少量（1个或几个）reduce子任务未完成。
    因为其处理的数据量和其他reduce差异过大。
    单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多。 最长时长远大于平均时长。

    数据倾斜的解决方案
    1、参数调节：
    hive.map.aggr=true
    Map 端部分聚合，相当于Combiner
    hive.groupby.skewindata=true
    有数据倾斜的时候进行负载均衡，当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，
    Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，
    这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；
    第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中
    （这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。

    2、 SQL语句调节：
    如何Join：
    关于驱动表的选取，选用join key分布最均匀的表作为驱动表
    做好列裁剪和filter操作，以达到两表做join的时候，数据量相对变小的效果。
    大小表Join：
    使用map join让小的维度表（1000条以下的记录条数） 先进内存。在map端完成reduce.
    大表Join大表：
    把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null值关联不上，处理后并不影响最终结果。
    count distinct大量相同特殊值
    count distinct时，将值为空的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果中加1。
    如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行union。
    group by维度过小：
    采用sum() group by的方式来替换count(distinct)完成计算。
    特殊情况特殊处理：
    在业务逻辑优化效果的不大情况下，有些时候是可以将倾斜的数据单独拿出来处理。最后union回去。

    典型的业务场景
    1、空值产生的数据倾斜
    场景：如日志中，常会有信息丢失的问题，比如日志中的 user_id，如果取其中的 user_id 和 用户表中的user_id 关联，会碰到数据倾斜的问题。
    解决方法1： user_id为空的不参与关联

    select * from log a
    join users b
    on a.user_id is not null
    and a.user_id = b.user_id
    union all
    select * from log a  where a.user_id is null;


    解决方法2 ：赋与空值分新的key值
    select *  from log a  left outer join users b  on case when a.user_id is null then concat(‘hive’,rand() ) else a.user_id end = b.user_id;

    结论：方法2比方法1效率更好，不但io少了，而且作业数也少了。解决方法1中 log读取两次，jobs是2。解决方法2 job数是1 。这个优化适合无效 id (比如 -99 ,’',null等)
     产生的倾斜问题。把空值的 key 变成一个字符串加上随机数，就能把倾斜的数据分到不同的reduce上 ,解决数据倾斜问题。

    2、不同数据类型关联产生数据倾斜
    场景：用户表中user_id字段为int，log表中user_id字段既有string类型也有int类型。当按照user_id进行两个表的Join操作时，默认的Hash操作会按int型的id来进行分配，
         这样会导致所有string类型id的记录都分配到一个Reducer中。
    解决方法：把数字类型转换成字符串类型
    select * from users a  left outer join logs b  on a.usr_id = cast(b.user_id as string)

    3、小表不小不大，怎么用 map join 解决倾斜问题
    使用 map join 解决小表(记录数少)关联大表的数据倾斜问题，这个方法使用的频率非常高，但如果小表很大，大到map join会出现bug或异常，这时就需要特别的处理。 以下例子:
    select * from log a  left outer join users b  on a.user_id = b.user_id;

    users 表有 600w+ 的记录，把 users 分发到所有的 map 上也是个不小的开销，而且 map join 不支持这么大的小表。如果用普通的 join，又会碰到数据倾斜的问题。
    解决方法：

    select /*+map join(x)*/* from log a  left outer join (
        select  /*+map join(c)*/d.*
            from ( select distinct user_id from log ) c
             join users d   on c.user_id = d.user_id
             ) x
             on a.user_id = b.user_id;

    假如，log里user_id有上百万个，这就又回到原来map join问题。所幸，每日的会员uv不会太多，有交易的会员不会太多，有点击的会员不会太多，有佣金的会员不会太多等等。
    所以这个方法能解决很多场景下的数据倾斜问题。
    总结
    使map的输出数据更均匀的分布到reduce中去，是我们的最终目标。由于Hash算法的局限性，按key Hash会或多或少的造成数据倾斜。大量经验表明数据倾斜的原因是人为的建表
    疏忽或业务逻辑可以规避的。在此给出较为通用的步骤：
    1、采样log表，哪些user_id比较倾斜，得到一个结果表tmp1。由于对计算框架来说，所有的数据过来，他都是不知道数据分布情况的，所以采样是并不可少的。
    2、数据的分布符合社会学统计规则，贫富不均。倾斜的key不会太多，就像一个社会的富人不多，奇特的人不多一样。所以tmp1记录数会很少。把tmp1和users做map join
       生成tmp2,把tmp2读到distribute file cache。这是一个map过程。
    3、map读入users和log，假如记录来自log,则检查user_id是否在tmp2里，如果是，输出到本地文件a,否则生成<user_id,value>的key,value对，假如记录来自member,
       生成<user_id,value>的key,value对，进入reduce阶段。
    4、最终把a文件，把Stage3 reduce阶段输出的文件合并起写到hdfs。

    如果确认业务需要这样倾斜的逻辑，考虑以下的优化方案：
    1、对于join，在判断小表不大于1G的情况下，使用map join
    2、对于group by或distinct，设定 hive.groupby.skewindata=true
    3、尽量使用上述的SQL语句调节进行优化

11.mysql和hive的区别

hive:读时模式   mysql:写时模式-检查格式

HiveQL是Hive查询语言，Hive可能与MySQL的方言最接近，但是两者还是存在显著性差异的。
Hive不支持行级插入操作、更新操作和删除操作。Hive也不支持事务。
Hive增加了在Hadoop背景下的可以提供更高性能的扩展，以及一些个性化的扩展，甚至还增加了一些外部程序

1.查询语言不同：hive是hql语言，mysql是sql语句；
2.数据存储位置不同：hive是把数据存储在hdfs上，而mysql数据是存储在自己的系统中；
3.数据格式：hive数据格式可以用户自定义，mysql有自己的系统定义格式；
4.数据更新：hive不支持数据更新，只可以读，不可以写，而sql支持数据更新；
5.索引：hive没有索引，因此查询数据的时候是通过mapreduce很暴力的把数据都查询一遍，也造成了hive查询数据速度很慢的原因，而mysql有索引；
6.延迟性：hive延迟性高，原因就是上边一点所说的，而mysql延迟性低；
7.数据规模：hive存储的数据量超级大，而mysql只是存储一些少量的业务数据；
8.底层执行原理：hive底层是用的mapreduce，而mysql是executor执行器；

http://www.cnblogs.com/guoruibing/articles/9894521.html

12.row_number() dense_rank() rank()的区别:
三个函数都是按照col1分组内从1开始排序
      ROW_NUMBER()  是没有重复值的排序(即使两条记录相同，序号也不重复的)，不会有同名次。 1 2 3 4
    DENSE_RANK() 是连续的排序，两个第二名仍然跟着第三名。 1 2 2 3
    RANK()     是跳跃排序，两个第二名下来就是第四名。  1 2 2 4
----------------------------------------------------------------------------
拓展:
最后再来一下数据库和数据仓储的区别:
 > 数据库是面向事务的设计，数据仓库是面向主题设计的。 数据库一般存储在线交易数据，数据仓库存储的一般是历史数据。
 > 数据库设计是尽量避免冗余，一般采用符合范式的规则来设计，数据仓库在设计是有意引入冗余，采用反范式的方式来设计。
 > 数据库是为捕获数据而设计，数据仓库是为分析数据而设计，它的两个基本的元素是维表和事实表。
  （维是看问题的角度，比如时间，部门，维表放的就是这些东西的定义，事实表里放着要查询的数据，同时有维的ID）
---------------------

1 hive的几种分区?
    Static Partition (SP) columns 静态分区:加载数据的时候指定分区的值。
    Dynamic Partition (DP) columns 动态分区:数据未知，根据分区的值确定创建分区。
    混合分区：静态加动态。
区别:
    1.DP列的指定方式与SP列相同 - 在分区子句中（ Partition关键字后面），唯一的区别是，DP列没有值，而SP列有值（ Partition关键字后面只有key没有value）
    2.在INSERT … SELECT …查询中，必须在SELECT语句中的列中最后指定动态分区列，并按PARTITION（）子句中出现的顺序进行排列
    3.所有DP列 - 只允许在非严格模式下使用。 在严格模式下，我们应该抛出一个错误
    4.如果动态分区和静态分区一起使用，必须是动态分区的字段在前，静态分区的字段在后。

2 hive的建表语法
    建表语句
        create external table test.emp1(
            id int comment 'This is col named id',
            name string comment 'This is col named name'
            )
         comment 'This is a external table named emp1 in test database'
         row format delimited fields terminated by ','
         location '/hive/emptable'
         ;

   建表语法
   create
      [external] table [if not exists] table_name
      [(col_name data_type [comment col_comment], ...)]
      [comment table_comment]
      [partitioned by (col_name data_type [comment col_comment], ...)]
      [clustered by (col_name, col_name, ...)
      [sorted by (col_name [asc|desc], ...)] into num_buckets buckets]
      [row format row_format]
      [stored as file_format]
      [location hdfs_path]

   create table if not exists stu(name string,age int)
   partitioned by (year int,month int)
   row format delimited fields terminated by ','
   LINES TERMINATED BY '\n'
   stored as orc;

3 hive的几种数据加载格式
    1 手动将数据添加到hive表对应的hdfs上的目录
    2 使用hive自带的命令往表中加载数据
       load data [local] inpath 'filepath' [overwrite] into table tablename;
    3 将查询结果添加到表中(会产生MR)
       INSERT (INTO|OVERWRITE) TABLE tablename1 select statement1 FROM fromstatement;
    4 传统的insert方式
       insert into table tablename [PARTITION (partcol1[=val1], partcol2[=val2] ...)] VALUES values_row [, values_row ...]
    5 直接从已有表的基础上构建新的表
        示例
        1.	根据test.emp表，构建一个全新的test.empid表
        hive> create table test.empid as select id from test.emp;
        2.	根据test.emp表，构建一个全新的test.empname表
        hive> create table test.empname as select name from test.emp;

老曹+版本:
    加载数据的语法：
    LOAD DATA [LOCAL] INPATH '/AA/BB/CC' INTO TABLE TABLE_NAME;
    加载数据的本质：
    将数据文件copy（不完全是copy）或者移动数据到表对应的目录下。

    加载数据：
    insert into table t_5
    select * from t_4
    where uid <5
    ;

    克隆表，不带数据：
    create table if not exists t_6 like t_5;

    克隆表带数据：
    create table if not exists t_7 like t_4
    location '/user/hive/warehouse/gp1808.db/t_4'
    ;
    location:后面接的一定是hdfs上的目录，不是文件

    克隆带数据：
    ##更灵活更常用的方式：
    ##跟创建表的方式一样，元数据和目录都会创建。
    create table if not exists t_8
    as
    select * from t_4
    where uid >1
    ;

    内部表和外部表的转换
    alter table t_newuser set TBLPROPERTIES('EXTERNAL'='TRUE');   ###true一定要大写
    alter table t_newuser set TBLPROPERTIES('EXTERNAL'='false');  ###false大小写都没关系

